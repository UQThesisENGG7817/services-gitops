apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    app: kube-prometheus-stack
    app.kubernetes.io/instance: kube-prometheus-stack
    app.kubernetes.io/part-of: kube-prometheus-stack
  name: awesome-prometheus-alerts
  # namespace: observability
spec:
  groups:
    - name: awesome-prometheus-alerts
      rules:
        ### TONY
        - alert: KubernetesPodMemoryOvercommit
          expr: (max_over_time(container_memory_working_set_bytes{container!=""}[15m]) / on(namespace,pod,container) group_left() kube_pod_container_resource_requests{resource="memory",job="kube-state-metrics"}) * 100 > 100
          for: 1h
          labels:
            severity: warning
            alert_group: awesome-prometheus-alerts
          annotations:
            summary: '*Container has overcommitted memory resource requests.*'
            description: |-
              Container `{{ $labels.container }}` in pod `{{ $labels.pod }}` has overcommited memory resource requests
              
              Consider to increase their `requests.memory` to actual memory usage. This should reduce the probability of pod rescheduling events and the probability of OOM events.
              
              `VALUE = {{ $value | printf "%.2f" }}%`
        - alert: KubernetesPodCPUOvercommit
          expr: (max_over_time(container_cpu_usage_seconds_total{container!=""}[15m]) / on(namespace,pod, container) group_left() kube_pod_container_resource_requests{resource="memory",job="kube-state-metrics"}) * 100 > 100
          for: 1h
          labels:
            severity: warning
            alert_group: awesome-prometheus-alerts
          annotations:
            summary: '*Container has overcommitted CPU resource requests.*'
            description: |-
              Container `{{ $labels.container }}` in pod `{{ $labels.pod }}` has overcommited CPU resource requests
              
              Consider to increase their `requests.cpu` to actual CPU usage. This should reduce the probability of pod rescheduling events and the probability of CPU starvation.
              
              `VALUE = {{ $value | printf "%.2f" }}%`
        - alert: KubernetesPodMemoryLowUtilization
          expr: (max_over_time(container_memory_working_set_bytes{container!=""}[15m]) / on(namespace,pod,container) group_left() kube_pod_container_resource_requests{resource="memory",job="kube-state-metrics"}) * 100 < 50
          for: 1h
          labels:
            severity: info
            alert_group: awesome-prometheus-alerts
          annotations:
            summary: '*Container has low memory utilization*'
            description: |-
              Container `{{ $labels.container }}` in pod `{{ $labels.pod }}` has memory utilization below than 50%
              
              Consider to to decrease their `resquests.memory` to actual memory usage. This should free up allocatable memory for pods with higher memory requirements.
              
              `VALUE = {{ $value | printf "%.2f" }}%`
        ### PROMETHEUS
        - alert: PrometheusTooManyRestarts
          expr: changes(process_start_time_seconds{job=~"prometheus|pushgateway|alertmanager"}[15m]) > 2
          for: 15m
          labels:
            severity: warning
            alert_group: awesome-prometheus-alerts
          annotations:
            summary: '*Prometheus too many restarts*'
            description: |-
              Prometheus has restarted more than twice in the last 15 minutes. It might be crashlooping.
              
              `VALUE = {{ $value }}`
        - alert: PrometheusAllTargetsMissing
          expr: sum by (job) (up) == 0
          for: 10m
          labels:
            severity: critical
            alert_group: awesome-prometheus-alerts
          annotations:
            summary: '*Prometheus all targets missing*'
            description: |-
              A Prometheus job does not have living target anymore.
              
              `VALUE = {{ $value }}`
        - alert: PrometheusConfigurationReloadFailure
          expr: prometheus_config_last_reload_successful != 1
          for: 15m
          labels:
            severity: warning
            alert_group: awesome-prometheus-alerts
          annotations:
            summary: '*Prometheus configuration reload failure*'
            description: |-
              Prometheus configuration reload error
              
              `VALUE = {{ $value }}`
        - alert: PrometheusAlertmanagerJobMissing
          expr: absent(up{job="kube-prometheus-stack-alertmanager"})
          for: 15m
          labels:
            severity: warning
            alert_group: awesome-prometheus-alerts
          annotations:
            summary: '*Prometheus AlertManager job missing*'
            description: |-
              A Prometheus AlertManager job has disappeared
              
              `VALUE = {{ $value }}`
        ### KUBERNETES
        - alert: KubernetesNodeMemoryPressure
          expr: kube_node_status_condition{condition="MemoryPressure",status="true"} == 1
          for: 10m
          labels:
            severity: critical
            alert_group: awesome-prometheus-alerts
          annotations:
            summary: '*Kubernetes Node memory pressure*'
            description: |-
              Node `{{ $labels.node }}` has MemoryPressure condition
              
              `VALUE = {{ $value }}`
        - alert: KubernetesNodeDiskPressure
          expr: kube_node_status_condition{condition="DiskPressure",status="true"} == 1
          for: 10m
          labels:
            severity: critical
            alert_group: awesome-prometheus-alerts
          annotations:
            summary: '*Kubernetes Node disk pressure*'
            description: |-
              Node `{{ $labels.node }}` has DiskPressure condition
              
              `VALUE = {{ $value }}`
        - alert: KubernetesNodeNetworkUnavailable
          expr: kube_node_status_condition{condition="NetworkUnavailable",status="true"} == 1
          for: 10m
          labels:
            severity: critical
            alert_group: awesome-prometheus-alerts
          annotations:
            summary: '*Kubernetes Node network unavailable*'
            description: |-
              Node `{{ $labels.node }}` has NetworkUnavailable condition
              
              `VALUE = {{ $value }}`
        - alert: KubernetesContainerOomKiller
          expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
          for: 10m
          labels:
            severity: critical
            alert_group: awesome-prometheus-alerts
          annotations:
            summary: '*Kubernetes Container oom killer*'
            description: |-
              Container `{{ $labels.container }}` in pod `{{ $labels.namespace }}/{{ $labels.pod }}` has been OOMKilled `{{ $value }}` times in the last 10 minutes.
              
              `VALUE = {{ $value }}`
        - alert: KubernetesCronjobSuspended
          expr: kube_cronjob_spec_suspend != 0
          for: 15m
          labels:
            severity: warning
            alert_group: awesome-prometheus-alerts
          annotations:
            summary: '*Kubernetes CronJob suspended*'
            description: |-
              CronJob `{{ $labels.namespace }}/{{ $labels.cronjob }}` is suspended
              
              `VALUE = {{ $value }}`
        - alert: KubernetesPersistentvolumeclaimPending
          expr: kube_persistentvolumeclaim_status_phase{phase="Pending"} == 1
          for: 10m
          labels:
            severity: critical
            alert_group: awesome-prometheus-alerts
          annotations:
            summary: '*Kubernetes PersistentVolumeClaim pending*'
            description: |-
              PersistentVolumeClaim `{{ $labels.namespace }}`/`{{ $labels.persistentvolumeclaim }}` is pending
              
              `VALUE = {{ $value }}`
        - alert: KubernetesVolumeOutOfDiskSpace
          expr: kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes * 100 < 10
          for: 10m
          labels:
            severity: critical
            alert_group: awesome-prometheus-alerts
          annotations:
            summary: '*Kubernetes Volume out of disk space*'
            description: |-
              Volume is almost full `(< 10% left)`
              
              `VALUE = {{ $value }}`
        - alert: KubernetesVolumeFullInFourDays
          expr: predict_linear(kubelet_volume_stats_available_bytes[6h:5m], 4 * 24 * 3600) < 0
          for: 1h
          labels:
            severity: warning
            alert_group: awesome-prometheus-alerts
          annotations:
            summary: '*Kubernetes Volume full in four days*'
            description: |-
              Volume under `{{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }}` is expected to fill up within four days. Currently `{{ $value | humanize }}` is available.
              
              `VALUE = {{ $value | humanize }}`
        - alert: KubernetesVolumeFullInADay
          expr: predict_linear(kubelet_volume_stats_available_bytes[6h:5m], 24 * 3600) < 0
          for: 30m
          labels:
            severity: critical
            alert_group: awesome-prometheus-alerts
          annotations:
            summary: '*Kubernetes Volume full in a day*'
            description: |-
              Volume under `{{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }}` is expected to fill up within a day. `Currently {{ $value | humanize }}` is available.
              
              `VALUE = {{ $value | humanize }}`
        - alert: KubernetesHpaMetricsUnavailability
          expr: kube_horizontalpodautoscaler_status_condition{status="false", condition="ScalingActive"} == 1
          for: 15m
          labels:
            severity: warning
            alert_group: awesome-prometheus-alerts
          annotations:
            summary: '*Kubernetes HPA metrics unavailability*'
            description: |-
              HPA `{{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler }}` is unable to collect metrics
              
              `VALUE = {{ $value }}`
        - alert: KubernetesHpaScaleInability
          expr: kube_horizontalpodautoscaler_status_condition{status="false", condition="AbleToScale"} == 1
          for: 15m
          labels:
            severity: warning
            alert_group: awesome-prometheus-alerts
          annotations:
            summary: '*Kubernetes HPA scale inability*'
            description: |-
              HPA `{{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler }}` is unable to scale
              
              `VALUE = {{ $value }}`
        - alert: KubernetesHpaUnderutilized
          expr: max(quantile_over_time(0.5, kube_horizontalpodautoscaler_status_desired_replicas[1d]) == kube_horizontalpodautoscaler_spec_min_replicas) by (horizontalpodautoscaler) > 3
          for: 15m
          labels:
            severity: info
            alert_group: awesome-prometheus-alerts
          annotations:
            summary: '*Kubernetes HPA underutilized*'
            description: |-
              HPA `{{ $labels.namespace }}/{{ $labels.horizontalpodautoscaler }}` is constantly at minimum replicas for 50% of the time. Potential cost saving here.
              
              `VALUE = {{ $value }}`
        - alert: KubePodImagePullBackOff
          annotations:
            description: >-
              ImagePullBackOff in container `{{$labels.container }}`` of pod `{{ $labels.namespace }}/{{ $labels.pod }}`` for longer than 10 minutes
            runbook_url: >-
              https://runbooks.prometheus-operator.dev/runbooks/kubernetes
            summary: '*Pod failed to pull at least one image*'
          expr: >-
            max_over_time(kube_pod_container_status_waiting_reason{reason="ImagePullBackOff",
            job="kube-state-metrics", namespace=~".*"}[5m]) >= 1
          for: 10m
          labels:
            severity: critical
            alert_group: awesome-prometheus-alerts
        # # Threshold should be customized for each cronjob name.
        # - alert: KubernetesCronjobTooLong
        #   expr: time() - kube_cronjob_next_schedule_time > 3600
        #   for: 0m
        #   labels:
        #     severity: warning
        #   annotations:
        #     summary: Kubernetes CronJob too long
        #     description: "CronJob {{ $labels.namespace }}/{{ $labels.cronjob }} is taking more than 1h to complete.\n  VALUE = {{ $value }}"
        # - alert: KubernetesJobSlowCompletion
        #   expr: kube_job_spec_completions - kube_job_status_succeeded - kube_job_status_failed > 0
        #   for: 12h
        #   labels:
        #     severity: critical
        #   annotations:
        #     summary: Kubernetes Job slow completion
        #     description: "Kubernetes Job {{ $labels.namespace }}/{{ $labels.job_name }} did not complete in time.\n  VALUE = {{ $value }}"
        ### ISTIO
        # - alert: IstioKubernetesGatewayAvailabilityDrop
        #   expr: min(kube_deployment_status_replicas_available{deployment="istio-ingressgateway", namespace="istio-system"}) without (instance, pod) < 2
        #   for: 5m
        #   labels:
        #     severity: critical
        #   annotations:
        #     summary: '*Istio Kubernetes gateway availability drop*'
        #     description: |-
        #       External Gateway pods have dropped. Inbound traffic will likely be affected.
              
        #       `VALUE = {{ $value }}`
        # - alert: IstioKubernetesGatewayInternalAvailabilityDrop
        #   expr: min(kube_deployment_status_replicas_available{deployment="istio-ingressgateway-internal", namespace="istio-system"}) without (instance, pod) < 2
        #   for: 5m
        #   labels:
        #     severity: critical
        #   annotations:
        #     summary: '*Istio Kubernetes gateway availability drop*'
        #     description: |-
        #       Internal Gateway pods have dropped. Inbound traffic will likely be affected.
              
        #       `VALUE = {{ $value }}`
        # - alert: IstioHighTotalRequestRate
        #   expr: sum(rate(istio_requests_total{reporter="destination"}[5m])) > 50
        #   for: 10m
        #   labels:
        #     severity: warning
        #   annotations:
        #     summary: '*Istio high total request rate*'
        #     description: |-
        #       Global request rate in the service mesh is unusually high.
              
        #       `VALUE = {{ $value }}`
        # - alert: IstioLowTotalRequestRate
        #   expr: sum(rate(istio_requests_total{reporter="destination"}[5m])) < 10
        #   for: 15m
        #   labels:
        #     severity: info
        #   annotations:
        #     summary: '*Istio low total request rate*'
        #     description: |-
        #       Global request rate in the service mesh is unusually low.
              
        #       `VALUE = {{ $value }}`
        # - alert: IstioHigh4xxErrorRate
        #   expr: sum(rate(istio_requests_total{reporter="destination", response_code=~"4.*"}[5m])) / sum(rate(istio_requests_total{reporter="destination"}[5m])) * 100 > 5
        #   for: 10m
        #   labels:
        #     severity: warning
        #   annotations:
        #     summary: '*Istio high 4xx error rate*'
        #     description: |-
        #       High percentage of HTTP 4xx responses in Istio (> 5%).
              
        #       `VALUE = {{ $value }}`
        # - alert: IstioHigh5xxErrorRate
        #   expr: sum(rate(istio_requests_total{reporter="destination", response_code=~"5.*"}[5m])) / sum(rate(istio_requests_total{reporter="destination"}[5m])) * 100 > 5
        #   for: 10m
        #   labels:
        #     severity: warning
        #   annotations:
        #     summary: '*Istio high 5xx error rate*'
        #     description: |-
        #       High percentage of HTTP 5xx responses in Istio (> 5%).
              
        #       `VALUE = {{ $value }}`
        # - alert: IstioHighRequestLatency
        #   expr: rate(istio_request_duration_milliseconds_sum{reporter="destination"}[1m]) / rate(istio_request_duration_milliseconds_count{reporter="destination"}[1m]) > 100
        #   for: 10m
        #   labels:
        #     severity: warning
        #   annotations:
        #     summary: '*Istio high request latency*'
        #     description: |-
        #       Istio average requests execution is HIGH.
              
        #       `VALUE = {{ $value }}ms`
        # - alert: IstioHighRequestLatency
        #   expr: rate(istio_request_duration_milliseconds_sum{reporter="destination"}[1m]) / rate(istio_request_duration_milliseconds_count{reporter="destination"}[1m]) > 250
        #   for: 5m
        #   labels:
        #     severity: critical
        #   annotations:
        #     summary: '*Istio high request latency*'
        #     description: |-
        #       Istio average requests execution is HIGH.
              
        #       `VALUE = {{ $value }}ms`
        # - alert: IstioLatency99Percentile
        #   expr: histogram_quantile(0.99, sum(rate(istio_request_duration_milliseconds_bucket[1m])) by (destination_canonical_service, destination_workload_namespace, source_canonical_service, source_workload_namespace, le)) > 1000
        #   for: 10m
        #   labels:
        #     severity: warning
        #   annotations:
        #     summary: '*Istio latency 99 percentile*'
        #     description: |-
        #       Istio 1% slowest requests are longer than 1000ms.
              
        #       `VALUE = {{ $value }}`
        ### ARGOCD
        # - alert: ArgocdServiceNotSynced
        #   expr: argocd_app_info{sync_status!="Synced"} != 0
        #   for: 15m
        #   labels:
        #     severity: info
        #   annotations:
        #     summary: ArgoCD service not synced
        #     description: "Service {{ $labels.name }} run by argo is currently not in sync.\n  `VALUE = {{ $value }}`"
        # - alert: ArgocdServiceUnhealthy
        #   expr: argocd_app_info{health_status!="Healthy"} != 0
        #   for: 15m
        #   labels:
        #     severity: info
        #   annotations:
        #     summary: ArgoCD service unhealthy
        #     description: "Service {{ $labels.name }} run by argo is currently not healthy.\n  `VALUE = {{ $value }}`"
        ### COREDNS
        - alert: CorednsPanicCount
          expr: increase(coredns_panics_total[1m]) > 0
          for: 5m
          labels:
            severity: critical
            alert_group: awesome-prometheus-alerts
          annotations:
            summary: '*CoreDNS Panic Count*'
            description: |-
              Number of CoreDNS panics encountered
              
              `VALUE = {{ $value }}`
        - alert: KubernetesPodCrashLooping
          expr: increase(kube_pod_container_status_restarts_total[5m]) > 1
          labels:
            severity: warning
            alert_group: awesome-prometheus-alerts
          annotations:
            summary: Kubernetes pod crash looping
            description: |-
              Pod `{{ $labels.namespace }}`/`{{ $labels.pod }}` is crash looping {{ $value }} times in total
              
              `VALUE = {{ $value }}`
        - alert: KubernetesPodCrashLooping
          expr: increase(kube_pod_container_status_restarts_total[5m]) > 5
          labels:
            severity: critical
            alert_group: awesome-prometheus-alerts
          annotations:
            summary: Kubernetes pod crash looping
            description: |-
              Pod `{{ $labels.namespace }}`/`{{ $labels.pod }}` is crash looping `{{ $value }}` times in total
              
              `VALUE = {{ $value }}`
        - alert: HighPodRestarts
          expr: kube_pod_container_status_restarts_total > 2
          labels:
            severity: warning
            alert_group: awesome-prometheus-alerts
          annotations:
            summary: Pod has restarted too many times
            description: |-
              The pod `{{ $labels.namespace }}`/`{{ $labels.pod }}` has restarted `{{ $value }}` times in total
              
              `VALUE = {{ $value }}`
        - alert: HighPodRestarts
          expr: kube_pod_container_status_restarts_total > 10
          labels:
            severity: critical
            alert_group: awesome-prometheus-alerts
          annotations:
            summary: Pod has restarted too many times
            description: |-
              The pod `{{ $labels.namespace }}`/`{{ $labels.pod }}` has restarted `{{ $value }}` times in total
              
              `VALUE = {{ $value }}`